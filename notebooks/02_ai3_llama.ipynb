{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaTokenizer\n",
    "from pefty_llama.modeling import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/vlialin/miniconda3/envs/pefty_llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /mnt/shared_home/vlialin/miniconda3/envs/pefty_llama/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "100%|██████████| 33/33 [00:07<00:00,  4.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "model = create_model(\"7b\", hf_path=\"../model_checkpoints/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  42 is the answer first of its own kind.\n",
      "The 2\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"42 is the answer\", return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "out1 = model.generate(input_ids, generation_length=10)\n",
    "print(tokenizer.decode(out1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pefty_llama.peft.ia3 import IA3\n",
    "model = IA3(model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  42 is the answer first of its own kind.\n",
      "The 2\n"
     ]
    }
   ],
   "source": [
    "out2 = model.generate(input_ids, generation_length=10)\n",
    "print(tokenizer.decode(out2[0]))\n",
    "assert torch.all(out1 == out2), \"At initialization, the model should produce the same output as the original model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 614,400\n"
     ]
    }
   ],
   "source": [
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3(\n",
       "  (base_model): LLaMAModel(\n",
       "    (model): LLaMAInnerModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LLaMALayer(\n",
       "          (self_attn): IA3Attention(\n",
       "            (q_proj): NoInitLinear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): NoInitLinear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): NoInitLinear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): NoInitLinear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): IA3MLP(\n",
       "            (gate_proj): NoInitLinear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): NoInitLinear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): NoInitLinear(in_features=11008, out_features=4096, bias=False)\n",
       "          )\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "    (lm_head): NoInitLinear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pefty_llama",
   "language": "python",
   "name": "pefty_llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
